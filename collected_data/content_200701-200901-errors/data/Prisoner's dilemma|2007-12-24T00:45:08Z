{{dablink|Many points in this article may be difficult to understand without a background in the elementary concepts of [[game theory]].}}<!--Replace with suitable template?-->
[[Image:Prison.jpg|right|frame|Will the two prisoners cooperate to minimize total loss of liberty, or will one of them, trusting the other to cooperate, betray him so as to go free?]]

In [[game theory]], the '''prisoner's dilemma''' (sometimes abbreviated '''PD''') is a type of non-[[zero-sum]] game in which two players may each "cooperate" with or "defect" (i.e., betray) the other player. In this game, as in all game theory, the only concern of each individual player ("prisoner") is maximizing his/her own payoff, without any concern for the other player's payoff.  The unique equilibrium for this game is a [[Pareto efficiency|Pareto-]]suboptimal solution&mdash;that is, rational choice leads the two players to both play ''defect'' even though each player's individual reward would be greater if they both played ''cooperate''. In equilibrium, each prisoner chooses to defect even though both would be better off by cooperating, hence the dilemma.

In the classic form of this game, cooperating is [[dominance (game theory)|strictly dominated]] by defecting, so that the only possible [[Nash equilibrium|equilibrium]] for the game is for all players to defect. In simpler terms, no matter what the other player does, one player will always gain a greater payoff by playing defect. Since in any situation playing ''defect'' is more beneficial than cooperating, all [[perfect rationality|rational]] players will play ''defect,'' all things being equal.

In the '''iterated prisoner's dilemma''' the game is [[repeated game|played repeatedly]]. Thus each player has an opportunity to "punish" the other player for previous non-cooperative play. Cooperation may then arise as an equilibrium outcome. The incentive to defect is overcome by the threat of punishment, leading to the possibility of a cooperative outcome. So if the game is infinitely repeated, cooperation may be a [[subgame perfect Nash equilibrium]] although both players defecting always remains an equilibrium and there are many other equilibrium outcomes.

==The classical prisoner's dilemma==
The Prisoner's Dilemma was originally framed by [[Merrill Flood]] and [[Melvin Dresher]] working at [[RAND]] in 1950.  [[Albert W. Tucker]] formalized the game with prison sentence payoffs and gave it the "Prisoner's Dilemma" name (Poundstone, 1992).

The classical prisoner's dilemma (PD) is as follows:

:Two suspects, A and B, are arrested by the police.  The police have insufficient evidence for a conviction, and, having separated both prisoners, visit each of them to offer the same deal: if one testifies for the prosecution against the other and the other remains silent, the betrayer goes free and the silent accomplice receives the full 10-year sentence.  If both remain silent, both prisoners are sentenced to only six months in jail for a minor charge.  If each betrays the other, each receives a five-year sentence.  Each prisoner must make the choice of whether to betray the other or to remain silent.  However, neither prisoner knows for sure what choice the other prisoner will make.  So this dilemma poses the question:  How should the prisoners act?

The dilemma can be summarized thus:

{| class="wikitable"
! !! Prisoner B Stays Silent !! Prisoner B Betrays
|-
! Prisoner A Stays Silent
| Each serves six months || Prisoner A serves ten years<br/>Prisoner B goes free
|-
! Prisoner A Betrays
| Prisoner A goes free<br/>Prisoner B serves ten years || Each serves five years
|}

The dilemma arises when one assumes that both prisoners only care about minimizing their own jail terms.  Each prisoner has two and only two options: either to co-operate with his accomplice and stay quiet, or to defect from their implied pact and betray his accomplice in return for a lighter sentence.  The outcome of each choice depends on the choice of the accomplice, but each prisoner must choose without knowing what his accomplice has chosen.

In deciding what to do in strategic situations, it is normally important to predict what others will do. This is not the case here. If you knew the other prisoner would stay silent, your best move is to betray as you then walk free instead of receiving the minor sentence.  If you knew the other prisoner would betray, your best move is still to betray, as you receive a lesser sentence than by silence. Betraying is a [[dominant strategy]]. The other prisoner reasons similarly, and therefore also chooses to betray. Yet by both defecting they get a lower payoff than they would get by staying silent. So rational, self-interested play results in each prisoner being worse off than if they had stayed silent. In more technical language, this demonstrates very elegantly that in a [[non-zero sum game]] a [[Nash Equilibrium]] need not be a [[Pareto optimum]].

Note that the paradox of the situation lies in that the prisoners are not defecting in hope that the other will not. Even when they both know the other to be rational and selfish, they will both play defect. Defect is what they will play ''no matter what'', even though they know fully well that the other player is playing defect as well and that they will both be better off with a different result. 

The "Stay Silent" and "Betray" [[Strategy (game theory)|strategies]] are also known as "don't confess" and "confess", or the more standard "cooperate" and "defect."

One [[Experimental economics|experiment]] based on the simple dilemma found that approximately 40% of participants cooperated (i.e., stayed silent).<ref>{{cite book |last=Tversky |first=Amos |title=Preference, Belief, and Similarity: Selected Writings |publisher=MIT Press |year=2004 |isbn=026270093X}}</ref>

==Generalized form==
We can expose the skeleton of the game by stripping it of the prisoner [[framing device]]. The generalized form of the game has been used frequently in [[experimental economics]]. The following rules give a typical realization of the game. 

There are two players and a banker. Each player holds a set of two cards: one printed with the word "Cooperate", the other printed with "Defect" (the standard terminology for the game). Each player puts one card face-down in front of the banker. By laying them face down, the possibility of a player knowing the other player's selection in advance is eliminated (although revealing one's move does not affect the dominance analysis<ref name="tell">A simple "[[Tell (poker)|tell]]" that partially or wholly reveals one player's choice — such as the Red player playing their Cooperate card face-up — does not change the fact that Defect is the dominant strategy. When one is considering the game itself, communication has no effect whatsoever. However, when the game is being played in real life considerations outside of the game itself may cause communication to matter. It is a point of utmost importance to the full implications of the dilemma that when we do not need to take into account external considerations, single-instance Prisoner's Dilemma is not affected in any way by communications.<p>Even in single-instance Prisoner's Dilemma, meaningful prior communication about issues external to the game could alter the play environment, by raising the possibility of enforceable side contracts or credible threats. For example, if the Red player plays their Cooperate card face-up and simultaneously reveals a binding commitment to blow the jail up if and only if Blue Defects (with additional payoff <span style="color: #009">-11</span>,<span style="color: #900">-10</span>), then Blue's Cooperation becomes dominant. As a result, players are screened from each other and prevented from communicating outside of the game.</p></ref>). At the end of the turn, the banker turns over both cards and gives out the payments accordingly.

If player 1 (red) defects and player 2 (blue) cooperates, player 1 gets the Temptation to Defect payoff of 5 points while player 2 receives the Sucker's payoff of 0 points. If both cooperate they get the Reward for Mutual Cooperation payoff of 3 points each, while if they both defect they get the Punishment for Mutual Defection payoff of 1 point. The checker board [[payoff matrix]] showing the payoffs is given below.

{| class="wikitable"
|+ Canonical PD payoff matrix
|
!scope="col" style="color: #900"|Cooperate
!scope="col" style="color: #900"|Defect
|-
!scope="row" style="color: #009"|Cooperate
|<span style="color: #009">3</span>, <span style="color: #900">3
|<span style="color: #009">0</span>, <span style="color: #900">5
|-
!scope="row" style="color: #009"|Defect
|<span style="color: #009">5</span>, <span style="color: #900">0</span>
|<span style="color: #009">1</span>, <span style="color: #900">1</span>
|}

In "win-lose" terminology the table looks like this:

{| class="wikitable"
|
!scope="col" style="color: #900"|Cooperate
!scope="col" style="color: #900"|Defect
|-
!scope="row" style="color: #009"|Cooperate
|<span style="color: #009">win</span>-<span style="color: #900">win</span>
|<span style="color: #009">lose much</span>-<span style="color: #900">win much</span>
|-
!scope="row" style="color: #009"|Defect
|<span style="color: #009">win much</span>-<span style="color: #900">lose much</span>
|<span style="color: #009">lose</span>-<span style="color: #900">lose</span>
|}

These point assignments are given arbitrarily for illustration. It is possible to generalize them. Let ''T'' stand for ''Temptation to defect'', ''R'' for ''Reward for mutual cooperation'', ''P'' for ''Punishment for mutual defection'' and ''S'' for ''Sucker's payoff''. The following inequalities must hold: 

''T'' > ''R'' > ''P'' > ''S''

In addition to the above condition, if the game is repeatedly played by two players, the following condition should be added.<ref>{{cite book
  | last = Dawkins
  | first = Richard
  | authorlink = Richard Dawkins
  | title = [[The Selfish Gene]]
  | publisher = [[Oxford University Press]]
  | date = 1989
  | id = ISBN 0-19-286092-5}} Page: 204 of Paperback edition</ref>

2 ''R'' > ''T'' + ''S''

If that condition does not hold, then full cooperation is not necessarily [[Pareto optimal]], as the players are collectively better off by having each player alternate between cooperate and defect.

These rules were established by cognitive scientist [[Douglas Hofstadter]] and form the formal canonical description of a typical game of Prisoner's Dilemma.

==The iterated prisoner's dilemma==
If two players play Prisoner's Dilemma more than once in succession (that is, having memory of at least one previous game), it is called iterated Prisoner's Dilemma. Amongst results shown by Nobel Prize winner [[Robert Aumann]] in his 1959 paper, rational players repeatedly interacting for indefinitely long games can sustain the cooperative outcome. Popular interest in the iterated prisoners dilemma (IPD) was kindled by [[Robert Axelrod]] in his book ''[[The Evolution of Cooperation]]'' (1984). In this he reports on a tournament he organized in which participants have to choose their mutual strategy again and again, and have memory of their previous encounters. Axelrod invited academic colleagues all over the world to devise computer strategies to compete in an [[IPD tournament]]. The programs that were entered varied widely in algorithmic complexity; initial hostility; capacity for forgiveness; and so forth.

Axelrod discovered that when these encounters were repeated over a long period of time with many players, each with different strategies, greedy strategies tended to do very poorly in the long run while more [[altruism|altruistic]] strategies did better, as judged purely by self-interest.  He used this to show a possible mechanism for the evolution of altruistic behaviour from mechanisms that are initially purely selfish, by [[natural selection]].

The best [[deterministic algorithm|deterministic]] strategy was found to be "[[Tit for Tat]]," which [[Anatol Rapoport]] developed and entered into the tournament. It was the simplest of any program entered, containing only four lines of BASIC, and won the contest. The strategy is simply to cooperate on the first iteration of the game; after that, the player does what his opponent did on the previous move. Depending on the situation, a slightly better strategy can be "Tit for Tat with forgiveness." When the opponent defects, on the next move, the player sometimes cooperates anyway, with a small probability (around 1%-5%).  This allows for occasional recovery from getting trapped in a cycle of defections.  The exact probability depends on the line-up of opponents.

By analysing the top-scoring strategies, Axelrod stated several conditions necessary for a strategy to be successful.

; Nice: The most important condition is that the strategy must be "nice", that is, it will not defect before its opponent does. Almost all of the top-scoring strategies were nice; therefore a purely selfish strategy will not "cheat" on its opponent, for purely utilitarian reasons first.
; Retaliating: However, Axelrod contended, the successful strategy must not be a blind optimist. It must sometimes retaliate. An example of a non-retaliating strategy is Always Cooperate. This is a very bad choice, as "nasty" strategies will ruthlessly exploit such softies.
; Forgiving: Another quality of successful strategies is that they must be forgiving. Though they will retaliate, they will once again fall back to cooperating if the opponent does not continue to play defects. This stops long runs of revenge and counter-revenge, maximizing points.
; Non-envious: The last quality is being non-envious, that is not striving to score more than the opponent (impossible for a ‘nice’ strategy, i.e., a 'nice' strategy can never score more than the opponent).

Therefore, Axelrod reached the [[Utopia]]n-sounding conclusion that selfish individuals for their own selfish good will tend to be nice and forgiving and non-envious. One of the most important conclusions of Axelrod's study of IPDs is that Nice guys can finish first.

The optimal (points-maximizing) strategy for the one-time PD game is simply defection; as explained above, this is true whatever the composition of opponents may be. However, in the iterated-PD game the optimal strategy depends upon the strategies of likely opponents, and how they will react to defections and cooperations. For example, consider a population where everyone defects every time, except for a single individual following the Tit-for-Tat strategy.  That individual is at a slight disadvantage because of the loss on the first turn.  In such a population, the optimal strategy for that individual is to defect every time.  In a population with a certain percentage of always-defectors and the rest being Tit-for-Tat players, the optimal strategy for an individual depends on the percentage, and on the length of the game. 

Deriving the optimal strategy is generally done in two ways:
# [[Bayesian game#Bayesian Nash equilibrium|Bayesian Nash Equilibrium]]: If the statistical distribution of opposing strategies can be determined (e.g. 50% tit-for-tat, 50% always cooperate) an optimal counter-strategy can be derived analytically.<ref name="bne">For example see the 2003 study [http://econ.hevra.haifa.ac.il/~mbengad/seminars/whole1.pdf  “Bayesian Nash equilibrium; a statistical test of the hypothesis”] for discussion of the concept and whether it can apply in real [[economic]] or strategic situations (from [[Tel Aviv University]]).</ref>
# [[Monte Carlo method|Monte Carlo]] simulations of populations have been made, where individuals with low scores die off, and those with high scores reproduce (a [[genetic algorithm]] for finding an optimal strategy).  The mix of algorithms in the final population generally depends on the mix in the initial population.  The introduction of mutation (random variation during reproduction) lessens the dependency on the initial population; empirical experiments with such systems tend to produce Tit-for-Tat players (see for instance Chess 1988), but there is no analytic proof that this will always occur.

Although Tit-for-Tat is considered to be the most robust basic strategy, a team from [[Southampton University]] in England (led by Professor Nicholas Jennings [http://www.ecs.soton.ac.uk/~nrj] and consisting of Rajdeep Dash, Sarvapali Ramchurn, Alex Rogers, Perukrishnen Vytelingum) introduced a new strategy at the 20th-anniversary Iterated Prisoner's Dilemma competition, which proved to be more successful than Tit-for-Tat.  This strategy relied on cooperation between programs to achieve the highest number of points for a single program.  The University submitted 60 programs to the competition, which were designed to recognize each other through a series of five to ten moves at the start.  Once this recognition was made, one program would always cooperate and the other would always defect, assuring the maximum number of points for the defector.  If the program realized that it was playing a non-Southampton player, it would continuously defect in an attempt to minimize the score of the competing program.  As a result,<ref name="southamptontrick">[http://www.prisoners-dilemma.com/results/cec04/ipd_cec04_full_run.html The 2004 Prisoner's Dilemma Tournament Results] show [[University of Southampton]]'s strategies in the first three places, despite having fewer wins and many more losses than the GRIM strategy. (Note that in a PD tournament, the aim of the game is not to “win” matches - that can easily be achieved by frequent defection). It should also be pointed out that even without implicit collusion between [[computer program|software strategies]] (exploited by the Southampton team) tit-for-tat is not always the absolute winner of any given tournament; it would be more precise to say that its long run results over a series of tournaments outperform its rivals. (In any one event a given strategy can be slightly better adjusted to the competition than tit-for-tat, but tit-for-tat is more robust). The same applies for the tit-for-tat-with-forgiveness variant, and other optimal strategies: on any given day they might not 'win' against a specific mix of counter-strategies.<p>An alternative way of putting it is using the Darinian [[Evolutionarily stable strategy|ESS]] simulation. In such a simulation Tit-for-Tat will almost always come to dominate, though nasty strategies will drift in and out of the population because a Tit-for-Tat population is penetratable by non-retaliating nice strategies which in turn are easy prey for the nasty strategies. Richard Dawkins showed that here no static mix of strategies form a stable equilibrium and the system will always oscillate between bounds.</p></ref> this strategy ended up taking the top three positions in the competition, as well as a number of positions towards the bottom.  

This strategy takes advantage of the fact that multiple entries were allowed in this particular competition, and that the performance of a team was measured by that of the highest-scoring player (meaning that the use of self-sacrificing players was a form of [[minmaxing]]). In a competition where one has control of only a single player, Tit-for-Tat is certainly a better strategy. Because of this new rule, this competition also has little theoretical significance when analysing single agent strategies as compared to Axelrod's seminal tournament. However, it provided the framework for analysing how to achieve cooperative strategies in multi-agent frameworks, especially in the presence of noise.  In fact, long before this new-rules tournament was played, Richard Dawkins in his book ''[[The Selfish Gene]]'' pointed out the possibility of such strategies winning if multiple entries were allowed, but remarked that most probably Axelrod would not have allowed them if they had been submitted. It also relies on circumventing rules about the prisoner's dilemma in that there is no communication allowed between the two players. When the Southampton programs engage in an opening "ten move dance" to recognize one another, this only reinforces just how valuable communication can be in shifting the balance of the game.

If an iterated PD is going to be iterated exactly N times, for some known constant N, then it is always optimal to defect in all rounds. The only possible [[Nash equilibrium]] is to always defect. The proof goes like this: one might as well defect on the last turn, since the opponent will not have a chance to punish the player. Therefore, both will defect on the last turn. Thus, the player might as well defect on the second-to-last turn, since the opponent will defect on the last no matter what is done, and so on. For cooperation to emerge the total number of rounds must be random, or at least unknown to the players. However, even in this case always defect is no longer a strictly dominant strategy, only a Nash equilibrium. Another odd case is "play forever" prisoner's dilemma. The game is repeated infinitely many times, and the player's score is the average (suitably computed).

The prisoner's dilemma game is fundamental to certain theories of human cooperation and trust. On the assumption that the PD can model transactions between two people requiring trust, cooperative behaviour in populations may be modelled by a multi-player, iterated, version of the game. It has, consequently, fascinated many scholars over the years. In 1975, Grofman and Pool estimated the count of scholarly articles devoted to it at over 2,000.  The iterated prisoner's dilemma has also been referred to as the "[[Peace war game|Peace-War game]]".<ref>Shy, O., 1996, ''[[industrial organization|Industrial Organization]]: Theory and Applications'', Cambridge, Mass.: The [[MIT]] Press.</ref>

====Learning psychology and game theory====
Where game players can learn to estimate the likelihood of other players defecting, their own behaviour is influenced by their experience of the others' behaviour.  Simple statistics show that inexperienced players are more likely to have had, overall, atypically good or bad interactions with other players.  If they act on the basis of these experiences (by defecting or cooperating more than they would otherwise) they are likely to suffer in future transactions.  As more experience is accrued a truer impression of the likelihood of defection is gained and game playing becomes more successful.  The early transactions experienced by immature players are likely to have a greater effect on their future playing than would such transactions affect mature players.  This principle goes part way towards explaining why the formative experiences of young people are so influential and why they are particularly vulnerable to bullying, sometimes ending up as bullies themselves.

The likelihood of defection in a population may be reduced by the experience of cooperation in earlier games allowing [[Trust (sociology)|trust]] to build up.<ref name="trust" />  Hence self-sacrificing behaviour may, in some instances, strengthen the moral fibre of a group.  If the group is small the positive behaviour is more likely to feed back in a mutually affirming way, encouraging individuals within that group to continue to cooperate.  This is allied to the twin dilemma of encouraging those people whom one would aid to indulge in behaviour that might put them at risk.  Such processes are major concerns within the study of [[reciprocal altruism]], [[group selection]], [[kin selection]] and [[Ethics (philosophy)|moral philosophy]].

===Rationality and super-rationality===

One resolution of the dilemma proposed by [[Douglas Hofstadter]] in his [[Metamagical Themas]] is to reject the definition of "rational" that led to the "rational" decision to defect. In this view, truly rational (or "[[superrationality|superrational]]") players take into account that the other person is (presumably) superrational, like them, and thus they cooperate. <!--Many authors and researchers seem unaware of this resolution of the dilemma, however. CONJECTURE --> This analysis of the one-shot game is in complete contradiction to classical game theory,
but according to this view follows naturally from the symmetry between the two players:
* an optimal strategy must be the same for both players (unlike the terms of the classical prisoner's game)
* the result must lie on the diagonal of the payoff matrix
* maximize return from solutions on the diagonal
* cooperate

===Morality===
While it is sometimes thought that [[morality]] must involve the constraint of self-interest, [[David Gauthier]] famously argues that co-operating in the prisoners dilemma on moral principles is consistent with self-interest and the axioms of game theory. It's most prudent to give up straightforward maximizing and instead adopt a disposition of constrained maximization, according to which one resolves to cooperate with all similarly disposed persons and defect on the rest. In other words, moral constraints are justified because they make us all better off, in terms of our preferences (whatever they may be). This form of [[contractarianism]] claims that good moral thinking is just an elevated and subtly strategic version of plain old means-end reasoning. Those that defect can be predicted because people are not completely opaque.

[[Douglas Hofstadter]] expresses a strong personal belief that 
the mathematical symmetry is reinforced by a moral symmetry, along the lines of the [[Kant]]ian [[categorical imperative]]: defecting in the hope that the other player cooperates is morally indefensible. 
If players treat each other as they would treat themselves, then off-diagonal results cannot occur.

==Real-life examples==
These particular examples, involving prisoners and bag switching and so forth, may seem contrived, but there are in fact many examples in human interaction as well as interactions in nature that have the same payoff matrix. The prisoner's dilemma is therefore of interest to the [[social science]]s such as [[economics]], [[politics]] and [[sociology]], as well as to the biological sciences such as [[ethology]] and [[evolutionary biology]]. Many natural processes have been abstracted into models in which living beings are engaged in endless games of Prisoner's Dilemma (PD). This wide applicability of the PD gives the game its substantial importance.

In [[political science]], for instance, the PD scenario is often used to illustrate the problem of two states engaged in an [[arms race]]. Both will reason that they have two options, either to increase [[military expenditure]] or to make an agreement to reduce weapons. Neither state can be certain that the other one will keep to such an agreement; therefore, they both incline towards military expansion. The [[paradox]] is that both states are acting [[Rationality|rational]]ly, but producing an apparently irrational result. This could be considered a [[corollary]] to [[deterrence theory]].

In [[sociology]] or [[criminology]], the PD may be applied to an actual dilemma facing two inmates. The game theorist Marek Kaminski, a former political prisoner, analysed the factors contributing to payoffs in the game set up by a prosecutor for arrested defendants (cf. [[#References|References]]). He concluded that while the PD is the ideal game of a prosecutor, numerous factors may strongly affect the payoffs and potentially change the properties of the game.

In program management and technology development, the PD applies to the relationship between the customer and the developer. Capt Dan Ward, an officer in the US Air Force, examined ''The Program Manager's Dilemma'' in an article published in Defense AT&L, a defense technology journal.<ref> Ward, D. (2004) [http://www.dau.mil/pubs/dam/05_06_2004/war-mj04.pdf The Program Manager's Dilemma The Program Manager's Dilemma] (Defense AT&L, Defense Acquisition University Press). </ref>

Another example concerns a well-known concept in [[cycling]] races, for instance in the [[Tour de France]]. Consider two cyclists halfway in a race, with the [[peloton]] (larger group) at great distance behind them. The two cyclists often work together (''mutual cooperation'') by sharing the tough load of the front position, where there is no shelter from the wind. If neither of the cyclists makes an effort to stay ahead, the peloton will soon catch up (''mutual defection''). An often-seen scenario is one cyclist doing the hard work alone (''cooperating''), keeping the two ahead of the peloton. In the end, this will likely lead to a victory for the second cyclist (''defecting'') who has an easy ride in the first cyclist's [[slipstream]].

Also in athletics, there is a widespread practice in high school wrestling where the participants intentionally lose unnaturally large amounts of weight so as to compete against lighter opponents.  In doing so, the participants are clearly not at their top level of physical and athletic fitness and yet often end up competing against the same opponents anyway, who have also followed this practice (''mutual defection'').  The result is a reduction in the level of competition.  Yet if a participant maintains their natural weight (''cooperating''), they will most likely compete against a stronger opponent who has lost considerable weight.  

Advertising is sometimes cited as a real life example of the prisoner’s dilemma.  When [[cigarette]] advertising was legal in the United States, competing cigarette manufacturers had to decide how much money to spend on advertising.  The effectiveness of Firm A’s advertising was partially determined by the advertising conducted by Firm B.  Likewise, the profit derived from advertising for Firm B is affected by the advertising conducted by Firm A.  If both Firm A and Firm B chose to advertise during a given period the advertising cancels out, receipts remain constant, and expenses increase due to the cost of advertising.  Both firms would benefit from a reduction in advertising.  However, should Firm B choose not to advertise, Firm A could benefit greatly by advertising. Nevertheless, the optimal amount of advertising by one firm depends on how much advertising the other undertakes. As the best strategy is dependent on what the other firm chooses there is no dominant strategy and this is not a prisoner's dilemma. The outcome is similar, though, in that both firms would be better off were they to advertise less than in the equilibrium. Sometimes cooperative behaviors do emerge in business situations.  For instance, cigarette manufacturers endorsed the creation of laws banning cigarette advertising, understanding that this would reduce costs and increase profits across the industry.<ref name="trust">This argument for the development of cooperation through trust is given in '' [[The Wisdom of Crowds]] '', where it is argued that long-distance [[capitalism]] was able to form around a nucleus of [[Religious Society of Friends|Quaker]]s, who always dealt honourably with their business partners. (Rather than defecting and reneging on promises – a phenomenon that had discouraged earlier long-term unenforceable overseas contracts). It is argued that dealings with reliable merchants allowed the [[meme]] for cooperation to spread to other traders, who spread it further until a high degree of cooperation became a profitable strategy in general [[commerce]] </ref>  This analysis is likely to be pertinent in many other business situations involving advertising.  

Large software projects under the [[GNU General Public License|GPL]] (such as [[Linux]]) can force cooperation in an otherwise standard PD situation.  Given a piece of [[Free Software]], you can study the (modifiable) source code and make improvements.  Then you can keep secret the improved version, i.e. keep the modified source code to yourself and distribute it in an unmodifiable binary form (defect).  Alternatively, you could share the improved version in a modifiable source code form (cooperate).  If everyone defects, then many are probably making exactly the same improvements.  For any software that is under the GPL, it is illegal to distribute only the unmodifiable form, including any changes made, thus forcing cooperation.  Hence, rival parties can all work on it and know that none will defect, and all share in the improvements made by the others.

Many real-life dilemmas involve multiple players.  Although metaphorical, [[Garrett Hardin|Hardin's]] [[tragedy of the commons]] may be viewed as an example of a multi-player generalization of the PD:  Each villager makes a choice for personal gain or restraint.  The collective reward for unanimous (or even frequent) defection is very low payoffs (representing the destruction of the "commons"). Such multi-player PDs are not formal as they can always be decomposed into a set of classical two-player games. The commons are not always exploited: [[William Poundstone]], in a book about the Prisoner's Dilemma (see References below), describes a situation in New Zealand where newspaper boxes are left unlocked. It is possible for someone to [[Excludability|take a paper without paying]] (''defecting'') but very few do, perhaps feeling that if they don't pay then nor will others, destroying the system. (Because there is [[causality|no mechanism]] for personal choice to influence others' decisions this widespread line of reasoning is called "[[magical thinking]]".)<ref name="magic">As well as being an explanation for the lack of petty-theft, [[magical thinking]] has been used to explain such things as voluntary [[voting]] (where a non-voter is considered a free rider). Potentially, it might be used to explain [[Wikipedia]] contributions: Text may be added under the assumption that if contributions are not made, then similar people will also fail to contribute (i.e. arguing from effect to cause). Alternatively, the explanation could depend on expected future actions (and not require a magical connection). Modelling future interactions requires the addition of the temporal dimension, as given in the [[prisoner's dilemma#The iterated prisoner's dilemma|Iterated prisoner’s dilemma section]].</ref> [[Newspaper]]s are less risky to distribute under the [[honour system]] than other consumables because taking more than one offers [[diminishing returns|very little extra benefit]].

The theoretical conclusion of PD is one reason why, in many countries, [[plea bargain]]ing is forbidden. Often, precisely the PD scenario applies: it is in the interest of both suspects to confess and testify against the other prisoner/suspect, even if each is innocent of the alleged crime. Arguably, the worst case is when only one party is guilty &mdash; here, the innocent one is unlikely to confess, while the guilty one is likely to confess and testify against the innocent.

== Related games ==
===Closed-bag exchange===
[[Douglas Hofstadter|Hofstadter]]<ref name="dh">{{cite book | first=Douglas R. | last=Hofstadter| authorlink=Douglas Hofstadter | title= [[Metamagical Themas]]: questing for the essence of mind and pattern | publisher= Bantam Dell Pub Group| year=1985 | id=ISBN 0-465-04566-9}} - see Ch.29 ''The Prisoner's Dilemma Computer Tournaments and the Evolution of Cooperation''.</ref> once suggested that people often find problems such as the PD problem easier to understand when it is illustrated in the form of a simple game, or trade-off.  One of several examples he used was "closed bag exchange":
: Two people meet and exchange closed bags, with the understanding that one of them contains money, and the other contains a purchase. Either player can choose to honour the deal by putting into his bag what he agreed, or he can defect by handing over an empty bag.

In this game, defection is always the best course, implying that rational agents will never play, and that "closed bag exchange" will be a [[missing market]] due to [[adverse selection]]. However, in this case both players cooperating and both players defecting actually give the same result, so chances of mutual cooperation, even in repeated games, are few.

In a variation, popular among [[hacker]]s and programmers, each bag-exchanging agent is given a memory (or access to a collective memory), and many exchanges are repeated over time. As noted, without this introduction of time and memory, not much is explained about the behaviour of actual systems and groups of people. The various choices that one would have to make can be seen. How big is the memory of each actor? What is the strategy of each actor? How are actors with various strategies distributed and what determines who interacts with whom and in what order? This discussion has not even mentioned the possibility of the formation (spontaneous or otherwise) of conglomerates of actors, negotiating their bag-exchanges collectively. And what about agents, who charge a fee for organising these bag exchanges? Or agents (journalists?) who collect and exchange information about the bag exchanges themselves?

===Friend or Foe?===
''[[Friend or Foe?]]'' is a game show that aired from 2002 to 2005 on the [[Game Show Network]] in the [[United States]].  It is an example of the prisoner's dilemma game tested by real people, but in an artificial setting.  On the game show, three pairs of people compete.  As each pair is eliminated, they play a game of Prisoner's Dilemma to determine how their winnings are split.  If they both cooperate (Friend), they share the winnings 50-50.  If one cooperates and the other defects (Foe), the defector gets all the winnings and the cooperator gets nothing.  If both defect, both leave with nothing.  Notice that the payoff matrix is slightly different from the standard one given above, as the payouts for the "both defect" and the "cooperate while the opponent defects" cases are identical.  This makes the "both defect" case a weak equilibrium, compared with being a strict equilibrium in the standard prisoner's dilemma.  If you know your opponent is going to vote Foe, then your choice does not affect your winnings.  In a certain sense, ''Friend or Foe'' has a payoff model between "Prisoner's Dilemma" and "[[Game of chicken|Chicken]]".

The payoff matrix is
{| class="wikitable"
|
!scope="col" style="color: #900"|Cooperate
!scope="col" style="color: #900"|Defect
|-
!scope="row" style="color: #009"|Cooperate
|<span style="color: #009">1</span>, <span style="color: #900">1
|<span style="color: #009">0</span>, <span style="color: #900">2
|-
!scope="row" style="color: #009"|Defect
|<span style="color: #009">2</span>, <span style="color: #900">0</span>
|<span style="color: #009">0</span>, <span style="color: #900">0</span>
|}

This payoff matrix was later used on the [[United Kingdom|British]] [[television]] programmes ''[[Shafted]]'' and ''[[Golden Balls]]''.

==See also==
{|
|
* [[Cellular automata]]
* [[Centipede game]]
* [[Conflict resolution research]]
* [[Diner's dilemma]]
* [[Evolutionarily stable strategy]]
|
* [[Folk theorem (game theory)]]
* [[Nash equilibrium]]
* [[Neuroeconomics]]
* [[Price equation]]
* [[Reciprocal altruism]]
|
* [[Rendezvous problem]]
* [[Superrationality]]
* [[Tit for tat]]
* [[Tragedy of the commons]]
* [[Tragedy of the anticommons]]
|
* [[Traveler's dilemma]]
* [[Trust (sociology)]]
* [[Social trap]]
* [[War of attrition (game)]]
* [[Zero-sum]]
|}

==Notes==
<!--This article uses the Cite.php citation mechanism. If you would like more information on how to add references to this article, please see http://meta.wikimedia.org/wiki/Cite/Cite.php -->
{{Reflist|2}}

==References==
{{Sourcesstart}}
* [[Robert Aumann]], “Acceptable points in general cooperative n-person games”, in R. D. Luce and A. W. Tucker (eds.), Contributions to the Theory 23 of Games IV, Annals of Mathematics Study 40, 287–324, Princeton University Press, Princeton NJ.
* [[Robert Axelrod|Axelrod, R.]] (1984). ''[[The Evolution of Cooperation]]''. ISBN 0-465-02121-2
* [[Kenneth Binmore]], Fun and Games.
* David M. Chess (1988). Simulating the evolution of behavior: the iterated prisoners' dilemma problem. Complex Systems, 2:663–670.
* [[Melvin Dresher|Dresher, M.]] (1961). ''The Mathematics of Games of Strategy: Theory and Applications''  Prentice-Hall, Englewood Cliffs, NJ.
* [[Merrill M. Flood|Flood, M.M.]] (1952).  Some experimental games. Research memorandum RM-789. [[RAND]] Corporation, Santa Monica, CA. <!--(Research Memoranda do not appear for sale at the RAND [http://www.rand.org/pubs/authors/f/flood_merrill_m.html store])-->
* Kaminski, Marek M. (2004) ''Games Prisoners Play''  Princeton University Press. ISBN 0-691-11721-7 http://webfiles.uci.edu/mkaminsk/www/book.html
* Poundstone, W. (1992) ''Prisoner's Dilemma'' Doubleday, NY NY.
* Greif, A. (2006). ''Institutions and the Path to the Modern Economy: Lessons from Medieval Trade.'' [[Cambridge University Press]], [[Cambridge]], UK.
* [[Anatol Rapoport|Rapoport, Anatol]] and Albert M. Chammah (1965). ''Prisoner's Dilemma''. [[University of Michigan Press]].
* Le, S. & Boyd, R. (In press). Evolutionary dynamics of the continuous iterated Prisoner's Dilemma, ''Journal of Theoretical Biology'' [http://www.sscnet.ucla.edu/anthro/faculty/boyd/LeBoydContPDJTB.pdf Full text]
* A. Rogers, R. K. Dash, S. D. Ramchurn, P. Vytelingum and N. R. Jennings (2007)  “Coordinating team players within a noisy iterated Prisoner’s Dilemma tournament”  Theoretical Computer Science 377 (1-3) 243-259. [http://users.ecs.soton.ac.uk/nrj/download-files/tcs07.pdf] 
{{Sourcesend}}

==External links==
*[http://plato.stanford.edu/entries/prisoner-dilemma/ Prisoner's Dilemma (Stanford Encyclopedia of Philosophy)]
*[http://www.nature.com/npp/journal/v31/n5/full/1300932a.html Effects of Tryptophan Depletion on the Performance of an Iterated Prisoner's Dilemma Game in Healthy Adults] - Nature Neuropsychopharmacology
*[http://www.egwald.ca/operationsresearch/prisonersdilemma.php Is there a "dilemma" in Prisoner's Dilemma] by Elmer G. Wiens
* [http://webfiles.uci.edu/mkaminsk/www/book.html "Games Prisoners Play"] - game-theoretic analysis of interactions among actual prisoners, including PD.
*Play an [http://people.bath.ac.uk/mk213/ipd/ iterated prisoner's dilemma game].
*Another version of the [http://www.gametheory.net/Web/PDilemma/ iterated prisoner's dilemma game]
*[http://www.paulspages.co.uk/hmd/ Iterated prisoner's dilemma game] applied to Big Brother TV show situation.
*[http://www.msri.org/ext/larryg/pages/15.htm The Bowerbird's Dilemma] The Prisoner's Dilemma in ornithology - mathematical cartoon by Larry Gonnick.
*[http://www.gohfgl.com/  Multiplayer game based on prisoner dilemma] Play Prisoner's Dilemma over IRC or internet  - by Axiologic Research.

===Selected documentaries===
*[http://video.google.com/videoplay?docid=8068309038544717701  Horizon documentary] - The Prisoner's Dilemma is introduced in [[Richard Dawkins]]' documentary ''[[Nice Guys Finish First]]'', [[BBC]] [[Horizon (BBC TV series)|''Horizon'' television series]] 
*[[The Trap (television documentary series)|The Trap: What Happened to Our Dream of Freedom]] - This BBC documentary by Adam Curtis starts from the idea that the theory of The Prisoner's Dilemma is the basis of political social control since the cold war.

== Further reading ==
* Plous, S. (1993). Prisoner's Dilemma or Perceptual Dilemma? ''Journal of Peace Research'', Vol. 30, No. 2, 163-179.


{{Spoken Wikipedia|Prisoners_Dilemma.ogg|2007-06-25}}
{{Game_theory}}
{{featured article}}

[[Category:Game theory]]
[[Category:Psychological theories]]
[[Category:Thought experiments]]

{{Link FA|es}}

[[ar:معضلة السجينين]]
[[bg:Дилема на затворника]]
[[ca:Dilema del presoner]]
[[cs:Vězňovo dilema]]
[[da:Fangernes dilemma]]
[[de:Gefangenendilemma]]
[[es:Dilema del prisionero]]
[[eo:Prizonula Dilemo]]
[[fr:Dilemme du prisonnier]]
[[ko:죄수의 딜레마]]
[[io:Karcerano-dilemo]]
[[it:Dilemma del prigioniero]]
[[he:דילמת האסיר]]
[[ka:ტუსაღის დილემა]]
[[lv:Cietumnieka dilemma]]
[[lt:Kalinio dilema]]
[[hu:Fogolydilemma]]
[[nl:Prisoner's dilemma]]
[[ja:囚人のジレンマ]]
[[no:Fangens dilemma]]
[[pl:Dylemat więźnia]]
[[pt:Dilema do prisioneiro]]
[[ro:Dilema prizonierului]]
[[ru:Дилемма заключённого]]
[[sl:Zapornikova dilema]]
[[fi:Vangin dilemma]]
[[sv:Fångarnas dilemma]]
[[vi:Song đề tù nhân]]
[[uk:Дилема в'язня]]
[[zh:囚徒困境]]