{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articles crawlled by class-Documentation\n",
    "\n",
    "### This is a method for collecting all the titles of a specific class.\n",
    "\n",
    "Given a class (FA, GA, A, B, C, Start or Stub), the crawller search is based in en.wikipedia.org/wiki/\"Desired_Class\"-Class_articles webpage, wich has all the categories that could have articles with the refered classification. Then the crawller goes to all of the category pages and get all the titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from xml.dom.minidom import parseString\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import json\n",
    "import file_manager as file_manager\n",
    "from checkpoint_manager import Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the attributes in the html\n",
    "  Let's assume that you have collected the html of the page. To indentify the elements in the page, like the \"div\" that as the links of the category pages, this two functions searches in the html for the desired \"id\" or \"class\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getElementById(elements,strId):\n",
    "\tfor element in elements:\n",
    "\t\tif element.hasAttribute('id') and element.getAttribute('id') == strId:\n",
    "\t\t\treturn element\n",
    "        \n",
    "def getElementsByClass(elements,strClass):\n",
    "    selected = []\n",
    "    for element in elements:\n",
    "        if element.hasAttribute('class') and element.getAttribute('class') == strClass:\n",
    "            selected.append(element)\n",
    "            \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request\n",
    "This function makes a request to the urls' page to get the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestCategory(urlToRequest):\n",
    "\tdataToResquest = {}\n",
    "\tr = requests.get(urlToRequest, data=dataToResquest)\n",
    "\tprint(\"Requisitando: \"+urlToRequest)\n",
    "\n",
    "\treturn parseString(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the next page and the subcategories\n",
    "The categories in Wikipedia can have subcategories links or titles. If the category has a lot of subcategories or titles, the page implements a pagination that creates a \"next page\" link. So the crawler has to get the subcategories and the next page links  and add they to the url_to_crawl list to be explored later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextPage_Link(domHTML):\n",
    "    mw_subcategories = getElementById(domHTML.getElementsByTagName(\"div\"),\"mw-subcategories\")\n",
    "    if mw_subcategories != None:\n",
    "        page_links = mw_subcategories.getElementsByTagName(\"a\")\n",
    "        if len(page_links) > 1:\n",
    "            for i in range(2):\n",
    "                if(page_links[i].childNodes[0].data == \"next page\"):\n",
    "                    if(\"href\" in page_links[i].attributes):\n",
    "                        return [page_links[i].attributes[\"href\"].value]\n",
    "    return []\n",
    "\n",
    "def getSubcategory_Links(domHTML,category):\n",
    "    subCats = getElementsByClass(domHTML.getElementsByTagName(\"div\"),\"CategoryTreeItem\")\n",
    "    to_crawl = []\n",
    "\n",
    "    for subcategory in subCats:\n",
    "        subcategory_spans = subcategory.getElementsByTagName(\"span\")\n",
    "        if  len(subcategory_spans)> 1:\n",
    "            has_data = subcategory_spans[1].childNodes[0].data\n",
    "            if(has_data != \"(empty)\" or has_data == \"â–º\"):\n",
    "                all_links = subcategory.getElementsByTagName(\"a\")\n",
    "                subcategory_link = all_links[0].attributes[\"href\"].value\n",
    "                if(subcategory_link.find(f\"Category:{category}\")>=0):\n",
    "                    to_crawl.append(subcategory_link)\n",
    "\n",
    "    return to_crawl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the crawler navigate and collect the titles\n",
    "\n",
    "&nbsp;\n",
    "First the crawler requests the category page and gets the html. Then it searches for links for future exploration, like a \"next page\" link for categories and the subcategories links.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "Now it's time for the titles. The titles and the \"next page\" link for titles are in a div whith the ID \"mw-pages\". After obtaining the div, the crawler verify if it is a \"next page\" link to save for future exploration. If it isn't a \"next page\" link it's a link to the Talk Page of the article, so simply getting the text of the link it's enought to know the article title and it is saved in a txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_cat_links(url_to_crawl,discovered_pages,log,category,checkpoint,output,get_sub_cats=False):\n",
    "    try:\n",
    "        domHTML = requestCategory(url_to_crawl)\n",
    "    except Exception as ex:\n",
    "        log(f\"Erro ao requisitar url: {url_to_crawl} ERRO: {ex}\")\n",
    "        return []\n",
    "\n",
    "    arrNewURLToCrawl = []\n",
    "\n",
    "#get the next page link and the subcategories links\n",
    "    arrNewURLToCrawl = getNextPage_Link(domHTML) + getSubcategory_Links(domHTML,category)\n",
    "    \n",
    "#write all article pages found save the next and previous link to be crawled\n",
    "    pages = getElementById(domHTML.getElementsByTagName(\"div\"),\"mw-pages\")\n",
    "    if(pages!=None):\n",
    "        arrSubCats = pages.getElementsByTagName(\"a\") \n",
    "        for catLink in arrSubCats:\n",
    "            link_text = catLink.childNodes[0].data.strip()\n",
    "            if(\"href\" in catLink.attributes):\n",
    "                link_url = catLink.attributes[\"href\"].value\n",
    "                if(link_url.find(f\"Category:{category}-Class_\")>=0):\n",
    "                    if(link_text!= \"previous page\"):\n",
    "                        arrNewURLToCrawl.append(link_url)\n",
    "                else:\n",
    "\n",
    "                    if(link_text not in discovered_pages):\n",
    "                        discovered_pages.append(link_text)\n",
    "                        file_manager.append_file(output,link_text)\n",
    "    return arrNewURLToCrawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main\n",
    "\n",
    "* **Notes:** \n",
    "  * The arguments were subsitute to compile the code in this notebook. Follow the instructions in the comments to run the algorithm in the terminal.\n",
    "  * The enable variable was added because exit() doesn't work for ending the program in Jupyter.\n",
    "  \n",
    "  \n",
    "&nbsp;\n",
    "While it has elements to crawl, the program gets the first element of the list, calls get_url_cat_links() that will collect the titles in the page and return the links in the page to be requested latter. The new links (the ones that is not in \"added_urls\" list) are saved in \"new_urls_to_crawl\". Then it is stored in \"added_urls\" wich is a list that stores all discovered urls, then the new urls are added to the \"urls_to_crawl\" to be explored later.\n",
    "\n",
    "* Struture\n",
    "  * \"added_urls\": Contains all the discovered urls;\n",
    "  * \"urls_to_crawl\": Contains the urls that are waiting to be requested;\n",
    "  * \"arr_urls\": Contais the discovered urls of this iteraction;\n",
    "  * \"new_urls_to_crawl\": Contais the discovered urls of this iteraction that are new (not in \"added_urls\").\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseted\n",
      "-----------------Starting the Crawler----------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # If you want FA, GA, A, B, C, Start or Stub articles, just put the name of the Class.\n",
    "    # Expecify with you want to reset the JSON: T or F.\n",
    "    # You also have to include the name of your JSON file to have a checkpoint.\n",
    "    # You always have to reset the JSON with the desired category to collect before starting the crawler. \n",
    "    # Example: python get_wiki_category.py A T checkpoint.json to reset the JSON.\n",
    "    #          python get_wiki_category.py A F checkpoint.json to start the crawler.\n",
    "    # Pause when want and your effort wont be lost!\n",
    "    # Good Crawling !!\n",
    "\n",
    "    category = \"A\"\n",
    "    reset = \"T\"\n",
    "    file_name = \"checkpoint.json\"\n",
    "\n",
    "    output = f\"articles_{category}\"\n",
    "    file_manager.create_file_if_does_not_exist(output)\n",
    "\n",
    "    checkpoint = Checkpoint(category,file_name)\n",
    "\n",
    "    enable = True\n",
    "    if(reset == \"T\"):\n",
    "        checkpoint.reset_Checkpoint()\n",
    "        print(\"Reseted\")\n",
    "        enable = False\n",
    "        # exit()\n",
    "\n",
    "    available_categories = [\"FA\",\"GA\",\"A\",\"B\",\"C\",\"Start\",\"Stub\"]\n",
    "    if(category in available_categories):\n",
    "        print(\"-----------------Starting the Crawler----------------------\")\n",
    "    else:\n",
    "        print(\"This code is for getting FA, GA, A, B, C, START and STUB articles only.\")\t\n",
    "        enable = False\n",
    "        # exit()\n",
    "        \n",
    "\n",
    "    added_urls, urls_to_crawl = checkpoint.load_Checkpoint()\n",
    "    discovered_pages = []\n",
    "    #parameters\n",
    "    domain = \"https://en.wikipedia.org\"\n",
    "    log = file_manager.create_logger(\"erros\")\n",
    "  \n",
    "    wasted_time = datetime.now() - datetime.now()\n",
    "    if(enable):\n",
    "        i = 1\n",
    "        while len(urls_to_crawl)>0:\n",
    "\n",
    "            time_before = datetime.now()\n",
    "\n",
    "            new_urls_to_crawl = []\n",
    "            current_url = urls_to_crawl.pop()\n",
    "            setNewUrls = set([])\n",
    "            arr_urls = None\n",
    "            while(arr_urls == None):\n",
    "                arr_urls = get_url_cat_links(current_url,discovered_pages,log,category, checkpoint, output, get_sub_cats=True)\n",
    "            setNewUrls = set(arr_urls)\n",
    "\n",
    "            [new_urls_to_crawl.append(domain+pageURL) for pageURL in setNewUrls if domain+pageURL not in added_urls]\n",
    "\n",
    "            added_urls = added_urls + new_urls_to_crawl\n",
    "\n",
    "            urls_to_crawl = urls_to_crawl + new_urls_to_crawl\n",
    "        # Save in the checkpoint\n",
    "            checkpoint.add(urls_to_crawl,\"urls_to_crawl\")\n",
    "            checkpoint.add(new_urls_to_crawl,\"visited\")\n",
    "\n",
    "            wasted_time = (wasted_time + (datetime.now() - time_before))/i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(\"URL (\"+str(i)+\")\"+current_url+\" crawled. URLs to crawl: \"+str(len(urls_to_crawl))+\" Time needed per URL: \"+str(wasted_time.microseconds/10**6)+\" seconds\" )\n",
    "\n",
    "            time.sleep(1)\n",
    "            i = i+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
